# Progress Summary
We aim to construct and train an implicit neural network-based preconditioner to accelerate solving the large linear systems associated with the Dirac equation in lattice gauge theory. Solving the linear systems determined by the complex gauge fields often requires effective preconditioners to apply the iterative solver efficiently. However, the choice and acquirement of the preconditioner could also bring excessive computational overhead. To address this issue, we aim to leverage the expressiveness of deep neural networks to construct linear operators from the complex gauge field so that they can be used as preconditioners without the explicit construction of matrices, reducing the memory demand for storing large matrices for both linear systems and preconditioners. The current challenges include effective representation learning of the linear system resulting from the complex gauge fields and efficient estimation of the condition number of the preconditioned system.

To achieve matrix-free learning and preconditioner creation, the current work focuses on two parts: extracting sufficient information from the original linear operator $D^{\dagger}D$, determined by the gauge fields, and producing a linear map (preconditioner) by minimizing a loss that corresponds to a faster convergence of the conjugate gradient solver or a lower condition number of the preconditioned linear system. For the first part, we applied the linear operator to $m$ (near) orthogonal vectors, $m\leq n$, $n$ being the dimension of the vectors, to project to different directions to retain the information. The orthogonality of the vectors is imposed during training by an additional loss term. For the second part, we have designed loss functions based on the residual norm of a conjugate gradient solver after a fixed number of iterations, the matrix-free estimation of condition numbers, and the calculation of $K$-condition numbers. The current results suggest that when $m$ is close to $n$, the orthogonal vectors sufficiently describe the linear operator, and neural network-produced preconditioners facilitated convergence on the training examples but were not helpful in the validation data (overfitting). 


The next steps include investigating the minimal effective $m$ for the orthogonal trainable vectors, repeating numerical experiments with more data, implementing and evaluating more reliable and efficient matrix-free methods to estimate the condition number, and exploring different types of neural network architectures and building blocks that are independent of the problem volume. Also, we plan to examine randomized frameworks, e.g., blendenpik, to establish a baseline and search for possible improvement using machine learning under the same framework.
