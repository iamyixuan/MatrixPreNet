{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "from NeuralPC.model.CNNs_flax import Encoder_Decoder \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_idx(length, key):\n",
    "    k = jax.random.PRNGKey(key)\n",
    "    idx = jax.random.permutation(k, length)\n",
    "    trainIdx = idx[:int(.6*length)]\n",
    "    valIdx = idx[-int(.4 * length):]\n",
    "    return trainIdx, valIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load('../../../datasets/Dirac/precond_data/config.l16-N200-b2.0-k0.276-unquenched-test.x.npy')\n",
    "data = jnp.asarray(data)\n",
    "data = jnp.transpose(data, [0, 2, 3, 1])\n",
    "\n",
    "# expoential transform\n",
    "data_exp = np.exp(1j * data)\n",
    "# print(data[0])\n",
    "dataReal = data_exp.real\n",
    "dataImag = data.imag\n",
    "dataComb = jnp.concatenate([dataReal, dataImag], axis=-1)\n",
    "print(dataComb.shape)\n",
    "\n",
    "# normalize the data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and val\n",
    "\n",
    "trainIdx, valIdx = split_idx(dataComb.shape[0], 42)\n",
    "trainData = dataComb[trainIdx]\n",
    "valData = dataComb[valIdx]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random').fit_transform(dataComb.reshape(dataComb.shape[0], -1))\n",
    "\n",
    "train_embed = tsne[trainIdx]\n",
    "val_embed = tsne[valIdx]\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_embed[:,0], train_embed[:, 1])\n",
    "ax.scatter(val_embed[:, 0], val_embed[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, index) :\n",
    "        return np.array(self.data[index])\n",
    "\n",
    "def create_dataLoader(data, batchSize, shuffle: bool):\n",
    "    dataset = Data(data)\n",
    "    loader = DataLoader(dataset, batch_size=batchSize, shuffle=shuffle)\n",
    "    return loader \n",
    "\n",
    "TrainLoader = create_dataLoader(np.array(trainData), 16, True)\n",
    "ValLoader = create_dataLoader(np.array(valData), 16, False)\n",
    "\n",
    "for v in ValLoader:\n",
    "    print(type(v))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "\n",
    "# print(model.tabulate(key, jnp.ones((1, 16, 16, 4))))\n",
    "# print(model.tabulate(jax.random.key(0), x))\n",
    "@jax.jit\n",
    "def MSE(params, x, y, train):\n",
    "    pred = model.apply(params, x, train=train)\n",
    "    return jnp.mean((y - pred) ** 2)\n",
    "\n",
    "# params = model.init(key, dataComb)\n",
    "variables = model.init(jax.random.key(0), dataComb, train=False)\n",
    "params = variables['params']\n",
    "batch_stats = variables['batch_stats']\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any\n",
    "\n",
    "def init_train_state(\n",
    "    model, random_key, shape, learning_rate\n",
    ") -> train_state.TrainState:\n",
    "    # Initialize the Model\n",
    "    variables = model.init(random_key, jnp.ones(shape), train=True)\n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    # Create a State\n",
    "    return TrainState.create(\n",
    "        apply_fn = model.apply,\n",
    "        tx=optimizer,\n",
    "        params=variables['params'],\n",
    "        batch_stats=variables['batch_stats']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState, batch: jnp.ndarray\n",
    "):\n",
    "    in_mat = batch\n",
    "    out_mat = batch\n",
    "\n",
    "\n",
    "    def loss_fn(params):\n",
    "        pred, updates = state.apply_fn({'params': params, 'batch_stats': state.batch_stats}, in_mat, train=True, mutable=['batch_stats'])\n",
    "        loss = jnp.mean((pred - out_mat)**2)\n",
    "        return loss, updates\n",
    "\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, updates), grads = gradient_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    state = state.replace(batch_stats=updates['batch_stats'])\n",
    "    return state, loss\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(\n",
    "    state, batch\n",
    "):\n",
    "    in_mat = batch\n",
    "    out_mat = batch\n",
    "\n",
    "    pred, updates = state.apply_fn({'params': params, 'batch_stats': state.batch_stats}, in_mat, train=False, mutable=['batch_stats'])\n",
    "    return jnp.mean((pred - out_mat)**2)\n",
    "\n",
    "def train_val(trainLoader, valLoader, state, epochs, verbose):\n",
    "    for ep in range(epochs):\n",
    "        # training\n",
    "        trainBatchLoss = []\n",
    "        for train_batch in trainLoader:\n",
    "            state, loss = train_step(state, train_batch.numpy())\n",
    "            trainBatchLoss.append(loss)\n",
    "        valBatchLoss = []        \n",
    "        for val_batch in valLoader:\n",
    "            vLoss = eval_step(state, val_batch.numpy())\n",
    "            valBatchLoss.append(vLoss)\n",
    "        if ep % 100 == 0 and verbose==True: \n",
    "            print('Epoch {}, train loss {:.4f} validation loss {:.4f}'.format(ep+1, np.mean(trainBatchLoss), np.mean(valBatchLoss)))\n",
    "        wandb.log({'trainLoss': np.mean(trainBatchLoss), 'valLoss': np.mean(valBatchLoss)})\n",
    "    return state\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from flax import struct\n",
    "\n",
    "# class TrainState(train_state.TrainState):\n",
    "#   metrics: MSE\n",
    "\n",
    "# def create_train_state(module, rng, learning_rate, momentum):\n",
    "#   \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "#   params = model.init(rng, jnp.ones([1, 16, 16, 4]))['params'] # initialize parameters by passing a template image\n",
    "#   tx = optax.sgd(learning_rate, momentum)\n",
    "#   return TrainState.create(\n",
    "#       apply_fn=model.apply, params=params, tx=tx,\n",
    "#       metrics=MSE)\n",
    "\n",
    "# @jax.jit\n",
    "# def train_step(state, batch):\n",
    "#   \"\"\"Train for a single step.\"\"\"\n",
    "#   def loss_fn(params):\n",
    "#     logits = state.apply_fn({'params': params}, batch['image'])\n",
    "#     loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "#         logits=logits, labels=batch['label']).mean()\n",
    "#     return loss\n",
    "#   grad_fn = jax.grad(loss_fn)\n",
    "#   grads = grad_fn(state.params)\n",
    "#   state = state.apply_gradients(grads=grads)\n",
    "#   return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trainIdx, valIdx = split_idx(dataComb.shape[0], 42)\n",
    "# trainData = dataComb[trainIdx]\n",
    "# valData = dataComb[valIdx]\n",
    "\n",
    "\n",
    "# opt_state = optimizer.init(params)\n",
    "# loss_grad_fn = jax.value_and_grad(MSE)\n",
    "# for i in range(epochs):\n",
    "#     trainLoss, grads = loss_grad_fn(params, trainData, trainData, True)\n",
    "#     valLoss = MSE(params, valData, valData, False)\n",
    "#     # Do the learning - updating the params.\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     wandb.log({'trainLoss': trainLoss, 'valLoss': valLoss})\n",
    "#     if i % 100 == 0:\n",
    "#         print('Loss step {}: '.format(i), trainLoss, 'validation loss {}'.format(valLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "h_ch = 64\n",
    "data = \"config_exp\"\n",
    "\n",
    "model = Encoder_Decoder(4, 4, h_ch, (3,3))\n",
    "# train the autoecoder\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"reconstructDiracConfig\",\n",
    "    name='BatchNorm-4-lr0.1',\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"h_ch\": h_ch,\n",
    "    })\n",
    "\n",
    "state = init_train_state(\n",
    "    model, key, (1, 16, 16, 4), learning_rate\n",
    ")\n",
    "final_state = train_val(TrainLoader, ValLoader, state, epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for the preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is element wise projection increasing the channels? i think so\n",
    "from NeuralPC.utils.dirac import DiracOperator\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "\n",
    "\n",
    "def conditionNum(L):\n",
    "    '''\n",
    "    L is the lower trangular matrix\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def PCG_loss(L, U1, b, kappa, steps):\n",
    "    '''\n",
    "        L : the neural network output, lower trangular preconditioner of shape (b, L^2, L^2, 2)\n",
    "        U1: gauge configuration of shape (b, L, L, 2).\n",
    "        steps: total steps for PCG to run.\n",
    "    '''\n",
    "    def runPCG(operator, b, precond=None):\n",
    "        x = jnp.zeros((1, L, L, 2))\n",
    "        x_sol = cg(A=operator, b=b, x0=x, M=precond, maxiter=steps)\n",
    "        return x_sol\n",
    "    \n",
    "    # fix the iteration step, calculate the residual b - Ax_sol and minimize the squared value.\n",
    "    operator = DiracOperator()\n",
    "    x_sol = runPCG(operator)\n",
    "    residual = b - operator(U1, kappa, x_sol)\n",
    "    return residual\n",
    "\n",
    "def BatchPCGLoss(L, U1, b, kappa, steps):\n",
    "    batchResidual = jax.vmap(PCG_loss, in_axes=[0, 1, 2, None, None])(L, U1, b, kappa, steps)\n",
    "    return jnp.mean(batchResidual**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2, 8, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 8, 8, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test block for cg\n",
    "\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "from NeuralPC.utils.dirac import DiracOperator\n",
    "from functools import partial\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "steps=100\n",
    "\n",
    "def random_b(key, shape):\n",
    "    # Generate random values for the real and imaginary parts\n",
    "    real_part = 1 - jax.random.uniform(key, shape)\n",
    "    imag_part = 1 - jax.random.uniform(jax.random.split(key)[1], shape)\n",
    "    # Combine the real and imaginary parts\n",
    "    complex_array = real_part + 1j * imag_part\n",
    "    return complex_array\n",
    "\n",
    "\n",
    "def runPCG(operator, b, precond=None):\n",
    "    x = jnp.zeros(b.shape).astype(b.dtype)\n",
    "    x_sol = cg(A=operator, b=b, x0=x, M=precond, maxiter=steps)\n",
    "    return x_sol\n",
    "\n",
    "U1 = np.load('../../../datasets/Dirac/precond_data/config.l8-N200-b2.0-k0.276-unquenched-test.x.npy')\n",
    "U1 = jnp.exp(1j*U1).astype(jnp.complex128)\n",
    "print(U1.shape)\n",
    "operator = partial(DiracOperator, U1=U1,kappa=0.276)\n",
    "y = operator(x=jnp.ones((200, 8, 8, 2)))\n",
    "\n",
    "b = random_b(jax.random.PRNGKey(0), (200, 8, 8, 2)).astype(jnp.complex128)\n",
    "# M = random_b(jax.random.PRNGKey(1), (8, 8)).astype(jnp.complex128)\n",
    "\n",
    "x_sol = runPCG(operator=operator, b=b, precond=None)\n",
    "x_sol[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the jax implementation of the Dirac operator\n",
    "\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "from NeuralPC.utils.dirac import example, exampleJAX\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "y_true = example('../../../datasets/Dirac/precond_data/config.l8-N200-b2.0-k0.276-unquenched-test.x.npy')\n",
    "y_jax = exampleJAX('../../../datasets/Dirac/precond_data/config.l8-N200-b2.0-k0.276-unquenched-test.x.npy')\n",
    "\n",
    "print(y_true.numpy() - y_jax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
